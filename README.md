# VectorSearchSimple
A simple vector search based on https://pub.aimind.so/understanding-vectors-and-building-a-rag-chatbot-with-azure-openai-and-langchain-cb4a885df21e

and an embedding model based on https://www.modular.com/ai-resources/introduction-to-embedding-models-a-beginners-guide

Vectorization: Turning Data into Numbers
Vectorization is the process of converting data into these numerical representations (vectors). In natural language processing, this often involves turning words or phrases into dense vectors of floating-point numbers. These vectors are designed to capture semantic meaning, allowing machines to process and compare text in a more nuanced way than simple string matching.

![image](https://github.com/user-attachments/assets/15c9f5cf-5cd2-4ed3-a8dd-c6fa31079d1e)


Embedding models are sophisticated feature learning methods that encode high-dimensional data into a lower-dimensional vector space. These representations allow easier manipulation and analysis of data by preserving key relationships and semantic information. By transforming complex data types into vectors, embedding models make it possible to uncover patterns and establish meaningful connections.

Types of Embedding Models
Embedding models are tailored to different data types. These include:

Word Embeddings: Tools like Word2Vec, GloVe, and FastText encode words into vector form, capturing contextual and semantic relationships.
Sentence Embeddings: Models such as Universal Sentence Encoder and Sentence-BERT encode entire sentences, aiding downstream NLP tasks like sentiment analysis and text classification.
Image Embeddings: Techniques leveraging Convolutional Neural Networks (CNNs) transform images into feature vectors, making tasks like image retrieval and similarity detection feasible.
Graph Embeddings: Methods such as Node2Vec and GraphSAGE encode nodes and edges within graph-structured data for applications like social network analysis and recommendation systems.
